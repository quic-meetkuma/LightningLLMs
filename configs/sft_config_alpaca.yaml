############################################
# @ Author: Meet Patel
# @ Create Time: 2025-08-12 17:20:53
# @ Modified by: Meet Patel
# @ Modified time: 2025-08-16 11:44:13
# @ Description:
############################################

# Model configuration
model:
  model_type: "hf"  # Hugging Face model
  auto_class_name: "AutoModelForCausalLM"
  model_name: "meta-llama/Llama-3.2-1B"  # Pretrained model name
  load_in_4bit: false
  use_peft: true
  peft_config:
    lora_r: 8
    lora_alpha: 16
    lora_dropout: 0.05
    target_modules: ["q_proj", "v_proj"]
    bias: "none"  # Options: none, all, lora_only
    task_type: "CAUSAL_LM"  # Options: CAUSAL_LM, SEQ_2_SEQ_LM, etc.
    peft_type: "LORA"  # Options: LORA, IA3, etc.

# Dataset configuration
dataset:
  tokenizer_name: "meta-llama/Llama-3.2-1B"
  dataset_type: "sft_dataset"
  dataset_name: "tatsu-lab/alpaca"
  train_split: "train"
  max_seq_length: null
  split_ratio: 0.8  # Ratio for train/test split, used when only train_split is provided
  test_split: "test"
  prompt_func: "LightningLLM.preprocessing.alpaca_func:create_alpaca_prompt"
  completion_template: "{output}"    # Model will be trained on this part.
  remove_samples_with_empty_columns: False
  group_by_length: True
  num_workers: 4
  pin_memory: True
  persistent_workers: True
  prefetch_factor: 1
  drop_last: False

# Training configuration
training:
  type: "sft"
  output_dir: "./training_results"
  overwrite_output_dir: False
  seed: 42

  do_eval: True
  eval_strategy: "epoch"
  # eval_steps: 100

  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 4
  num_train_epochs: 5
  max_steps: -1

  log_level: "info"
  log_on_each_node: True
  logging_strategy: "steps"
  logging_steps: 10

  save_strategy: "epoch"
  # save_steps: 100   # If 'save_strategy' is 'steps' then it will be used.
  save_total_limit: 5
  metric_for_best_model: "eval_loss"

  dtype: "fp16"
  completion_only_loss: True
  report_to: "trackio"

  # Uncomment if running in Notebook
  # disable_tqdm: True

  # Uncomment below fsdp block to enable FSDP training
  # fsdp: "full_shard"
  # fsdp_config: "./configs/accelerate/fsdp_config.yaml"
  # fsdp_config: "./configs/accelerate/fsdp_tp_parallelism_config.yaml"

  # Uncomment below deepspeed block to enable DeepSpeed training
  # deepspeed_config:
  #   abc: 10

  # Uncomment below DDP block to enable DDP training and configure DDP params
  ddp_config:
    ddp_backend: "qccl"
    ddp_find_unused_parameters: False
    ddp_bucket_cap_mb: 25
    ddp_broadcast_buffers: null
    ddp_timeout: 1800

  # Uncomment below to explicitly run on CPU
  use_cpu: False

  # Uncomment and populate to resume training
  # resume_from_checkpoint: "./abc"
  # restore_callback_states_from_checkpoint: True

  # Uncomment below block to enable and configure gradient checkpointing.
  gradient_checkpointing: False
  gradient_checkpointing_kwargs:
    preserve_rng_state : True
    use_reenrant: False

  torch_compile: True
  include_tokens_per_second: True
  include_num_input_tokens_seen: True
  average_tokens_across_devices: True

# Optimizer configuration
optimizers:
  optimizer_name: "adamw"
  lr: 3e-4
  weight_decay: 0.01


# “linear” → transformers.get_linear_schedule_with_warmup
# “cosine” → transformers.get_cosine_schedule_with_warmup
# “cosine_with_restarts” -->transformers.get_cosine_with_hard_restarts_schedule_with_warmup
# “polynomial” → transformers.get_polynomial_decay_schedule_with_warmup
# “constant” → transformers.get_constant_schedule
# “constant_with_warmup” → transformers.get_constant_schedule_with_warmup
# “inverse_sqrt” → transformers.get_inverse_sqrt_schedule

scheduler:
  scheduler_name: "cosine"
  warmup_steps: 100   # warmup_steps or warmup_ratio
  warmup_ratio: 0.1

# # Loss function configuration
# loss_functions:
#   cross_entropy:  # Options: cross_entropy, label_smoothing
#     label_smoothing: 0.1
#     reduction: "mean"
#     ignore_index: -100
#     weight: 1.0

callbacks:
  early_stopping:
    early_stopping_patience: 3
    early_stopping_threshold: 0.001
  tensorboard:

